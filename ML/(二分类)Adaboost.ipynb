{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937e919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier # 我们把max_depth设为1就当作我们的一个简单的分类器Gm(x)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a8728f",
   "metadata": {},
   "source": [
    "# Boost\n",
    "\n",
    "“装袋”（bagging）和“提升”（boost）是构建组合模型的两种最主要的方法，所谓的组合模型是由多个基本模型构成的模型，组合模型的预测效果往往比任意一个基本模型的效果都要好。\n",
    "\n",
    "- 装袋：每个基本模型由从总体样本中随机抽样得到的不同数据集进行训练得到，通过重抽样得到不同训练数据集的过程称为装袋。\n",
    "\n",
    "- 提升：每个基本模型训练时的数据集采用不同权重，针对上一个基本模型分类错误的样本增加权重，使得新的模型重点关注误分类样本\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "AdaBoost是AdaptiveBoost的缩写，表明该算法是具有适应性的提升算法。\n",
    "\n",
    "算法的步骤如下：\n",
    "\n",
    "1）给每个训练样本（$x_{1},x_{2},….,x_{N}$）分配权重，初始权重$w_{1}$均为1/N。\n",
    "\n",
    "2）针对带有权值的样本进行训练，得到模型$G_m$（初始模型为G1）。\n",
    "\n",
    "3）计算模型$G_m$的误分率$e_m=\\sum_{i=1}^Nw_iI(y_i\\not= G_m(x_i))$\n",
    "\n",
    "4）计算模型$G_m$的系数$\\alpha_m=0.5\\log[(1-e_m)/e_m]$\n",
    "\n",
    "5）根据误分率e和当前权重向量$w_m$更新权重向量$w_{m+1}$。\n",
    "\n",
    "6）计算组合模型$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x_i)$的误分率。\n",
    "\n",
    "7）当组合模型的误分率或迭代次数低于一定阈值，停止迭代；否则，回到步骤2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "222b7adb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46499067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 0. ],\n",
       "       [4.9, 3. , 0. ],\n",
       "       [4.7, 3.2, 0. ],\n",
       "       [4.6, 3.1, 0. ],\n",
       "       [5. , 3.6, 0. ],\n",
       "       [5.4, 3.9, 0. ],\n",
       "       [4.6, 3.4, 0. ],\n",
       "       [5. , 3.4, 0. ],\n",
       "       [4.4, 2.9, 0. ],\n",
       "       [4.9, 3.1, 0. ],\n",
       "       [5.4, 3.7, 0. ],\n",
       "       [4.8, 3.4, 0. ],\n",
       "       [4.8, 3. , 0. ],\n",
       "       [4.3, 3. , 0. ],\n",
       "       [5.8, 4. , 0. ],\n",
       "       [5.7, 4.4, 0. ],\n",
       "       [5.4, 3.9, 0. ],\n",
       "       [5.1, 3.5, 0. ],\n",
       "       [5.7, 3.8, 0. ],\n",
       "       [5.1, 3.8, 0. ],\n",
       "       [5.4, 3.4, 0. ],\n",
       "       [5.1, 3.7, 0. ],\n",
       "       [4.6, 3.6, 0. ],\n",
       "       [5.1, 3.3, 0. ],\n",
       "       [4.8, 3.4, 0. ],\n",
       "       [5. , 3. , 0. ],\n",
       "       [5. , 3.4, 0. ],\n",
       "       [5.2, 3.5, 0. ],\n",
       "       [5.2, 3.4, 0. ],\n",
       "       [4.7, 3.2, 0. ],\n",
       "       [4.8, 3.1, 0. ],\n",
       "       [5.4, 3.4, 0. ],\n",
       "       [5.2, 4.1, 0. ],\n",
       "       [5.5, 4.2, 0. ],\n",
       "       [4.9, 3.1, 0. ],\n",
       "       [5. , 3.2, 0. ],\n",
       "       [5.5, 3.5, 0. ],\n",
       "       [4.9, 3.6, 0. ],\n",
       "       [4.4, 3. , 0. ],\n",
       "       [5.1, 3.4, 0. ],\n",
       "       [5. , 3.5, 0. ],\n",
       "       [4.5, 2.3, 0. ],\n",
       "       [4.4, 3.2, 0. ],\n",
       "       [5. , 3.5, 0. ],\n",
       "       [5.1, 3.8, 0. ],\n",
       "       [4.8, 3. , 0. ],\n",
       "       [5.1, 3.8, 0. ],\n",
       "       [4.6, 3.2, 0. ],\n",
       "       [5.3, 3.7, 0. ],\n",
       "       [5. , 3.3, 0. ],\n",
       "       [7. , 3.2, 1. ],\n",
       "       [6.4, 3.2, 1. ],\n",
       "       [6.9, 3.1, 1. ],\n",
       "       [5.5, 2.3, 1. ],\n",
       "       [6.5, 2.8, 1. ],\n",
       "       [5.7, 2.8, 1. ],\n",
       "       [6.3, 3.3, 1. ],\n",
       "       [4.9, 2.4, 1. ],\n",
       "       [6.6, 2.9, 1. ],\n",
       "       [5.2, 2.7, 1. ],\n",
       "       [5. , 2. , 1. ],\n",
       "       [5.9, 3. , 1. ],\n",
       "       [6. , 2.2, 1. ],\n",
       "       [6.1, 2.9, 1. ],\n",
       "       [5.6, 2.9, 1. ],\n",
       "       [6.7, 3.1, 1. ],\n",
       "       [5.6, 3. , 1. ],\n",
       "       [5.8, 2.7, 1. ],\n",
       "       [6.2, 2.2, 1. ],\n",
       "       [5.6, 2.5, 1. ],\n",
       "       [5.9, 3.2, 1. ],\n",
       "       [6.1, 2.8, 1. ],\n",
       "       [6.3, 2.5, 1. ],\n",
       "       [6.1, 2.8, 1. ],\n",
       "       [6.4, 2.9, 1. ],\n",
       "       [6.6, 3. , 1. ],\n",
       "       [6.8, 2.8, 1. ],\n",
       "       [6.7, 3. , 1. ],\n",
       "       [6. , 2.9, 1. ],\n",
       "       [5.7, 2.6, 1. ],\n",
       "       [5.5, 2.4, 1. ],\n",
       "       [5.5, 2.4, 1. ],\n",
       "       [5.8, 2.7, 1. ],\n",
       "       [6. , 2.7, 1. ],\n",
       "       [5.4, 3. , 1. ],\n",
       "       [6. , 3.4, 1. ],\n",
       "       [6.7, 3.1, 1. ],\n",
       "       [6.3, 2.3, 1. ],\n",
       "       [5.6, 3. , 1. ],\n",
       "       [5.5, 2.5, 1. ],\n",
       "       [5.5, 2.6, 1. ],\n",
       "       [6.1, 3. , 1. ],\n",
       "       [5.8, 2.6, 1. ],\n",
       "       [5. , 2.3, 1. ],\n",
       "       [5.6, 2.7, 1. ],\n",
       "       [5.7, 3. , 1. ],\n",
       "       [5.7, 2.9, 1. ],\n",
       "       [6.2, 2.9, 1. ],\n",
       "       [5.1, 2.5, 1. ],\n",
       "       [5.7, 2.8, 1. ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面就是：首先：我们要创建我们的数据：\n",
    "# def create_data():\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data= iris.data, columns= iris.feature_names)\n",
    "data['labels'] = iris.target\n",
    "data.columns = ['sepal length', 'sepal width', 'petal length', 'petal width',\"target_name\"]\n",
    "# 下面就是我们只留两个特征！！！\n",
    "data = np.array(data.iloc[:100, [0,1,-1]])\n",
    "\n",
    "# 下面就是建构我们的3类变成两类：\n",
    "for i in range(len(data)):\n",
    "    if data[i,-1] == 0:\n",
    "        data[i,-1]== -1     \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d9369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57d3aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面就是划分我们的x：\n",
    "x = data[:,:2]\n",
    "# 左闭右开！！！\n",
    "y = data[:,-1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef842962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面就是将我们的x和y分成训练集和测试集：\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33ea01bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x14d9ffb1090>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 下面就是我们把我们的数据画出来看一下：\n",
    "plt.scatter(x[y == 1][:,0], x[y==1][:,1],c = 'blue',label = '0')\n",
    "plt.scatter(x[y==0][:,0],x[y==0][:,1], c= 'red', label ='1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3341e9bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们设置我们的简单的分类器：\n",
    "# 首先是实例化！！！\n",
    "\n",
    "weak_cla = DecisionTreeClassifier(max_depth =1, criterion= \"entropy\")\n",
    "weak_cla.fit(x_train, y_train)\n",
    "score = weak_cla.score(x_test,y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e14ca352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 10.0.1 (20240210.2158)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"258pt\" height=\"176pt\"\n",
       " viewBox=\"0.00 0.00 257.75 175.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 171.5)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-171.5 253.75,-171.5 253.75,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"182.75,-167.5 67,-167.5 67,-93.5 182.75,-93.5 182.75,-167.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"124.88\" y=\"-150.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x[0] &lt;= 5.45</text>\n",
       "<text text-anchor=\"middle\" x=\"124.88\" y=\"-133.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.996</text>\n",
       "<text text-anchor=\"middle\" x=\"124.88\" y=\"-117.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 80</text>\n",
       "<text text-anchor=\"middle\" x=\"124.88\" y=\"-100.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [37, 43]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"115.75,-57.5 0,-57.5 0,0 115.75,0 115.75,-57.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"57.88\" y=\"-40.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.503</text>\n",
       "<text text-anchor=\"middle\" x=\"57.88\" y=\"-23.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 36</text>\n",
       "<text text-anchor=\"middle\" x=\"57.88\" y=\"-7.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [32, 4]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.5,-93.21C94.79,-84.71 88.7,-75.64 82.96,-67.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"85.99,-65.33 77.51,-58.98 80.18,-69.24 85.99,-65.33\"/>\n",
       "<text text-anchor=\"middle\" x=\"71.85\" y=\"-77.21\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"249.75,-57.5 134,-57.5 134,0 249.75,0 249.75,-57.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"191.88\" y=\"-40.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.511</text>\n",
       "<text text-anchor=\"middle\" x=\"191.88\" y=\"-23.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 44</text>\n",
       "<text text-anchor=\"middle\" x=\"191.88\" y=\"-7.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [5, 39]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M149.25,-93.21C154.96,-84.71 161.05,-75.64 166.79,-67.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.57,-69.24 172.24,-58.98 163.76,-65.33 169.57,-69.24\"/>\n",
       "<text text-anchor=\"middle\" x=\"177.9\" y=\"-77.21\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x14d9fff8510>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "from sklearn import tree\n",
    "tree_data = tree.export_graphviz(weak_cla)\n",
    "graph = graphviz.Source(tree_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "867e5c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_cla.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff7b9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
       "       1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = weak_cla.predict(x_train)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c341bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = weak_cla.score(x_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "861f4767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b7663ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4ad202e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [2, 4]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1],[2]])*np.array([1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7bcf4",
   "metadata": {},
   "source": [
    "# 上面是我们的单个分类器的下效果：我们下面就是我们的集成学习的adaboost："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b281cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost: # 没有继承！！！不用加任何括号！！！别和神经网络搞混了！！！\n",
    "    def __init__(self, num_cls = 5): # 我们下面就是我们的构造方法！！！我们的弱分类器的个数我们的初始化默认设为100\n",
    "        self.num_cls = num_cls\n",
    "    \n",
    "    def init_arg(self, x, y): # 下面就是我们的构造我们的参数的过程！！！\n",
    "        # 我们把我们的x和y传进我们的这个类里面：\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        # 首先是我们的初始化我们的参数：首先是对于我们的每个样本的权重参数！！1\n",
    "        # 我们的每个参数设置成一样的就行了！！！！我们的参数是一个由样本个数为长度的一个向量！！！\n",
    "        self.weight = np.ones(x.shape[0])/x.shape[0]\n",
    "        \n",
    "        # 下面就是我们的装弱分类器单体的列表：\n",
    "        self.clfs = []\n",
    "        # 下面就是我们的每个分类的权重矩阵\n",
    "        self.alphas = []\n",
    "    \n",
    "    \n",
    "    # 下面就是我们的训练的过程：\n",
    "    def fit(self, x, y):\n",
    "        # 首先我们先初始化我们的参数：\n",
    "        self.init_arg(x, y)\n",
    "        \n",
    "        # 下面就是我们的遍历我们的全部所有分类器的过程：【我们已经设置好了我们的分类器就是100个！！！】\n",
    "        # 我们倒是乘权重再累加的项数就是一百项！！！\n",
    "        # 我们到时候的各个分类器权重就是有100个！！！【和书上的“误差小于某个值就停止不一样！！！”】\n",
    "        # 【所以】我们下面以生成的弱分类器的个数来停止我们的循环，训练过程中，我们不做权重与弱分类器对应相乘再求和，\n",
    "        # 再来求误差的操作！！！\n",
    "        for m in range(self.num_cls):\n",
    "            \n",
    "            # 首先是构造我们的弱分类器：\n",
    "            clf = DecisionTreeClassifier(max_depth=1)\n",
    "            \n",
    "            # 然后：我们拿我们的弱分类器去训练我们的数据；\n",
    "            # 【注意！！！】下面加个sample_weight = self.weight就是可以体现我们的分类器更加\n",
    "            # 对于我们的分错的数据更加专注【真的把各个数据的权重读进去了！！！】\n",
    "            # 书上的例子体现的不明显！！！直接选的分类器，但是我们复现的时候要体现！！！\n",
    "            # 【我们下次生成的弱分类器就【自然去】会有限去处理我们的权重高（上次分错类）的数据！！！】\n",
    "            clf.fit(x, y, sample_weight= self.weight)\n",
    "            \n",
    "            # 下面我们就是要得到我们的当前（参考了我们的各个样本的本次循环时的权重）\n",
    "            # 生成的弱分类器的学习误差率：【这个学习误差率可以算出我们的权重】\n",
    "            # 我们现在先要预测出我们的x数据：\n",
    "            p = clf.predict(x) \n",
    "            # 我们现在的【p和y都是一样长度的【列】向量了】！！！因为我们的x每个样本是按照列来排序的！！！\n",
    "            # 下面我们就可以得到我们的分类误差率了【直接矩阵相乘！！！】\n",
    "            em = np.dot(self.weight, (p!=y))\n",
    "            \n",
    "            \n",
    "            # 下面我们就可以将我们的【对应分类器的权重】alpha求出来了【我们的alphas列表是一个M（这里是num_cls）维的行向量】\n",
    "            # 我们的alpha是一个数！！！我们每生成一个弱分类器就算一次！！！\n",
    "            # 我们的weight视频一个向量！！！！我们每次算完一个新的弱分类器就更新一次\n",
    "            alpha = 1/2*np.log((1-em)/em)\n",
    "            \n",
    "            \n",
    "            # 下面我们就可以更新我们的权重了：\n",
    "            # 首先是规范化因子（用于归一化）【使得我们的每个分类器的权重矩阵的和都为1！！！】\n",
    "            # 【注意！！！】我们的alpha这时候是一个常数！！！\n",
    "            # 我们的这里的这个y乘p是对应元素对应相乘！！！\n",
    "            Z_m = sum(self.weight * np.exp(-alpha * y *p))\n",
    "            self.weight = self.weight * np.exp(-alpha * y *p)/Z_m\n",
    "            \n",
    "            \n",
    "            # 下面就是将我们算出来的各个分类器的权重和各个分类器记载在两个列表中就行了：\n",
    "            self.alphas.append(alpha)\n",
    "            self.clfs.append(clf)\n",
    "            \n",
    "        return 'DONE' # 我们已经生成完num_clf个弱分类器了，也找到num_clf个权重\n",
    "    \n",
    "    # 下面就是我们的总模型预测我们的标签的过程：\n",
    "    def predict(self, x):\n",
    "        # 就是我们拿一部分训练集样本出来：我们要可以做出预测\n",
    "        len1 = x.shape[0]\n",
    "        \n",
    "        # 接下来就是初始化我们的对这len1个数据的分别的预测的数组\n",
    "        pre_f_sum_x = np.zeros(len1)\n",
    "        \n",
    "        # 【注意！！！】我们的adaboost的弱分类器本质上是在训练数据上通过优势结合：\n",
    "        # 【从而】达到完美的划分效果！！！所以输入给我们adaboost的训练数据一定是多条的！！！【也没什么并行计算！！！】\n",
    "        # 【所以注意！！！】我们这里可以是只输入进来一个x数据，更可以是输入进来【好几条数据】\n",
    "        # 【最终奥义！！！】我们是将多个弱分类器化为了一个大的分类器！！！我们可以当作一个分类器来看！！！\n",
    "        # 当作一个大的分类器就和最开始的初始化一个分类器的看的方法是一样的了！！！\n",
    "        # 我们这个这个弱分类器是决策树！！！我们总的分类器就可以当作一个大的决策树！！！\n",
    "        # 本来就是对多条数据来划分的！！！（不是回归。。。） \n",
    "        # 当然：我们这个决策树已经训练好了，一条数据也可以找到分到的类\n",
    "        # 下面我们开始做加和：\n",
    "        for alpha,clf in zip(self.alphas, self.clfs):\n",
    "            pre_f_sum_x  += alpha * clf.predict(x)\n",
    "        \n",
    "        # 返回值别忘了\n",
    "        return np.sign(pre_f_sum_x)\n",
    "        \n",
    "    # 下面就是找到我们的得分！！！\n",
    "    def score(self, xtest, ytest):\n",
    "        pre = self.predict(xtest) # 注意！！！我们的这个pre数组和我们的ytest是同一个形状的！！！\n",
    "        sum_right = sum(pre == ytest)\n",
    "        return sum_right/(xtest.shape[0]) # 我们这样就找到了我们的正确率！！！【在测试集中】\n",
    "    \n",
    "    def different_weights(self):\n",
    "        # 我们还可以返回我们的权重：\n",
    "        return self.weight,self.alphas,self.clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fad8b14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e3048cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DONE'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost1 = Adaboost(num_cls=100)\n",
    "adaboost1.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d121e9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3d43a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adaboost1.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad3ab970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "425fa5c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.54617786e-02, 2.54617786e-02, 1.17360235e-04, 3.01040404e-03,\n",
       "        3.48628851e-03, 2.54617786e-02, 2.54617786e-02, 2.84438241e-04,\n",
       "        2.54617786e-02, 2.54617786e-02, 2.54617786e-02, 2.84438241e-04,\n",
       "        2.54617786e-02, 3.36297807e-05, 3.48628851e-03, 2.54617786e-02,\n",
       "        4.12191826e-04, 3.36297807e-05, 2.54617786e-02, 2.54617786e-02,\n",
       "        2.54617786e-02, 2.54617786e-02, 1.17360235e-04, 2.54617786e-02,\n",
       "        2.54617786e-02, 2.54617786e-02, 4.12191826e-04, 2.84438241e-04,\n",
       "        2.54617786e-02, 4.12191826e-04, 3.36297807e-05, 2.54617786e-02,\n",
       "        2.84438241e-04, 3.01040404e-03, 2.84438241e-04, 2.54617786e-02,\n",
       "        3.36297807e-05, 2.54617786e-02, 4.12191826e-04, 2.84438241e-04,\n",
       "        2.54617786e-02, 2.84438241e-04, 2.54617786e-02, 2.54617786e-02,\n",
       "        2.84438241e-04, 2.54617786e-02, 2.54617786e-02, 2.54617786e-02,\n",
       "        1.17360235e-04, 1.17360235e-04, 2.54617786e-02, 3.01040404e-03,\n",
       "        2.84438241e-04, 2.54617786e-02, 1.43845510e-03, 2.54617786e-02,\n",
       "        2.54617786e-02, 1.17360235e-04, 2.54617786e-02, 3.48628851e-03,\n",
       "        2.54617786e-02, 2.54617786e-02, 4.12191826e-04, 2.84438241e-04,\n",
       "        4.12191826e-04, 2.54617786e-02, 3.36297807e-05, 2.54617786e-02,\n",
       "        3.36297807e-05, 3.36297807e-05, 2.84438241e-04, 3.36297807e-05,\n",
       "        1.07180769e-03, 2.54617786e-02, 4.12191826e-04, 1.07180769e-03,\n",
       "        2.54617786e-02, 1.43845510e-03, 2.54617786e-02, 1.07180769e-03]),\n",
       " [1.032727649852548,\n",
       "  0.8852680177028326,\n",
       "  1.0670288947725524,\n",
       "  0.9556130927796008,\n",
       "  1.2498361285013302,\n",
       "  1.4390479389314739,\n",
       "  1.3945669442069333,\n",
       "  1.3945669442069335,\n",
       "  1.3945669442069337,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934,\n",
       "  1.394566944206934],\n",
       " [DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1),\n",
       "  DecisionTreeClassifier(max_depth=1)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost1.different_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd61e93",
   "metadata": {},
   "source": [
    "# sklearn.ensemble.AdaBoostClassifier\n",
    "\n",
    "- algorithm：这个参数只有AdaBoostClassifier有。主要原因是scikit-learn实现了两种Adaboost分类算法，SAMME和SAMME.R。两者的主要区别是弱学习器权重的度量，SAMME使用了和我们的原理篇里二元分类Adaboost算法的扩展，即用对样本集分类效果作为弱学习器权重，而SAMME.R使用了对样本集分类的预测概率大小来作为弱学习器权重。由于SAMME.R使用了概率度量的连续值，迭代一般比SAMME快，因此AdaBoostClassifier的默认算法algorithm的值也是SAMME.R。我们一般使用默认的SAMME.R就够了，但是要注意的是使用了SAMME.R， 则弱分类学习器参数base_estimator必须限制使用支持概率预测的分类器。SAMME算法则没有这个限制。\n",
    "\n",
    "- n_estimators： AdaBoostClassifier和AdaBoostRegressor都有，就是我们的弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是50。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。\n",
    "\n",
    "-  learning_rate:  AdaBoostClassifier和AdaBoostRegressor都有，即每个弱学习器的权重缩减系数ν\n",
    "\n",
    "- base_estimator：AdaBoostClassifier和AdaBoostRegressor都有，即我们的弱分类学习器或者弱回归学习器。理论上可以选择任何一个分类或者回归学习器，不过需要支持样本权重。我们常用的一般是CART决策树或者神经网络MLP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53d50ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(learning_rate=0.5, n_estimators=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(learning_rate=0.5, n_estimators=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(learning_rate=0.5, n_estimators=100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.5)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b9b9b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
